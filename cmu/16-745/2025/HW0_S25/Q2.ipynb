{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is how we activate an environment in our current directory\n",
    "import Pkg; Pkg.activate(@__DIR__)\n",
    "\n",
    "# instantate this environment (download packages if you haven't)\n",
    "Pkg.instantiate();\n",
    "\n",
    "using Test, LinearAlgebra\n",
    "import ForwardDiff as FD \n",
    "import FiniteDiff as FD2 \n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-neutral",
   "metadata": {},
   "source": [
    "# Q2: Newton's Method (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-telling",
   "metadata": {},
   "source": [
    "## Part (a): Newton's method in 1 dimension (8pts)\n",
    "First let's look at a nonlinear function, and label where this function is equal to 0 (a root of the function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "let \n",
    "    x = 2:0.1:4;\n",
    "    y = sin.(x) .* x.^2\n",
    "    plot(x,y,label = \"function of interest\")\n",
    "    plot!(x,0*x,linestyle = :dash, color = :black,label = \"\")\n",
    "    xlabel!(\"x\")\n",
    "    ylabel!(\"f(x)\")\n",
    "    scatter!([pi],[0],label = \"zero\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-taste",
   "metadata": {},
   "source": [
    "We are now going to use Newton's method to numerically evaluate the argument $x$ where this function is equal to zero. To make this more general, let's define a residual function,\n",
    "$$ r(x) = \\sin(x)x^2. $$\n",
    "We want to drive this residual function to be zero (aka find a root to $r(x)$). To do this, we start with an initial guess at $x_k$, and approximate our residual function with a first-order Taylor expansion:\n",
    "$$ r(x_k + \\Delta x) \\approx r(x_k) + \\bigg[ \\frac{\\partial r}{\\partial x}\\bigg\\rvert_{x_k} \\bigg] \\Delta x. $$ \n",
    "We now want to find the root of this linear approximation. In other words, we want to find a $\\Delta x$ such that $r(x_k + \\Delta x) = 0$. To do this, we simply re-arrange:\n",
    "$$ \\Delta x = -\\bigg[ \\frac{\\partial r}{\\partial x}\\bigg\\rvert_{x_k} \\bigg]^{-1}r(x_k). $$ \n",
    "We can now increment our estimate of the root with the following:\n",
    "$$ x_{k+1} = x_k + \\Delta x$$\n",
    "We have now described one step of Netwon's method. We started with an initial point, linearized the residual function, and solved for the $\\Delta x$ that drove this linear approximation to zero. We keep taking Newton steps until $r(x_k)$ is close enough to zero for our purposes (usually not hard to drive below 1e-10). \n",
    "\n",
    "\n",
    "Julia tip: `x=A\\b` solves linear systems of the form $Ax = b$ whether $A$ is a matrix or a scalar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    X = newtons_method_1d(x0, residual_function; max_iters)\n",
    "\n",
    "Given an initial guess x0::Float64, and `residual_function`, \n",
    "use Newton's method to calculate the zero that makes \n",
    "residual_function(x) ≈ 0. Store your iterates in a vector \n",
    "X and return X[1:i]. (first element of the returned vector\n",
    "should be x0, last element should be the solution)\n",
    "\"\"\"\n",
    "\n",
    "function newtons_method_1d(x0::Float64, residual_function::Function; max_iters = 10)::Vector{Float64}\n",
    "    # return the history of iterates as a 1d vector (Vector{Float64})\n",
    "    # consider convergence to be when abs(residual_function(X[i])) < 1e-10 \n",
    "    # at this point, trim X to be X = X[1:i], and return X \n",
    "\n",
    "    X = zeros(max_iters)\n",
    "    X[1] = x0 \n",
    "    \n",
    "    for i = 1:max_iters \n",
    "        \n",
    "        # TODO: Newton's method here\n",
    "        \n",
    "        # return the trimmed X[1:i] after you converge \n",
    "\n",
    "    end\n",
    "    error(\"Newton did not converge\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"2a\" begin\n",
    "    # residual function \n",
    "    residual_fx(_x) = sin(_x)*_x^2\n",
    "    \n",
    "    x0 = 2.8 \n",
    "    X = newtons_method_1d(x0, residual_fx; max_iters = 10)\n",
    "    R = residual_fx.(X) # the . evaluates the function at each element of the array\n",
    "    \n",
    "    @test abs(R[end]) < 1e-10\n",
    "    \n",
    "    # plotting\n",
    "    display(plot(abs.(R),yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "         yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "         title = \"Convergence of Newton's Method (1D case)\",label = \"\"))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-independence",
   "metadata": {},
   "source": [
    "## Part (b): Newton's method in multiple variables (8 pts)\n",
    "We are now going to use Newton's method to solve for the zero of a multivariate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    X = newtons_method(x0, residual_function; max_iters)\n",
    "\n",
    "Given an initial guess x0::Vector{Float64}, and `residual_function`, \n",
    "use Newton's method to calculate the zero that makes \n",
    "norm(residual_function(x)) ≈ 0. Store your iterates in a vector \n",
    "X and return X[1:i]. (first element of the returned vector\n",
    "should be x0, last element should be the solution)\n",
    "\"\"\"\n",
    "\n",
    "function newtons_method(x0::Vector{Float64}, residual_function::Function; max_iters = 10)::Vector{Vector{Float64}}\n",
    "    # return the history of iterates as a vector of vectors (Vector{Vector{Float64}})\n",
    "    # consider convergence to be when norm(residual_function(X[i])) < 1e-10 \n",
    "    # at this point, trim X to be X = X[1:i], and return X \n",
    "\n",
    "    X = [zeros(length(x0)) for i = 1:max_iters]\n",
    "    X[1] = x0 \n",
    "    \n",
    "    for i = 1:max_iters \n",
    "        \n",
    "        # TODO: Newton's method here\n",
    "        \n",
    "        # return the trimmed X[1:i] after you converge \n",
    "\n",
    "    end\n",
    "    error(\"Newton did not converge\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"2b\" begin \n",
    "    # residual function \n",
    "    r(x) = [sin(x[3] + 0.3)*cos(x[2]- 0.2) - 0.3*x[1];\n",
    "            cos(x[1]) + sin(x[2]) + tan(x[3]);\n",
    "            3*x[1] + 0.1*x[2]^3]\n",
    "    \n",
    "    x0 = [.1;.1;0.1]\n",
    "    X = newtons_method(x0, r; max_iters = 10)\n",
    "    R = r.(X) # the . evaluates the function at each element of the array\n",
    "\n",
    "    Rp = [[abs(R[i][ii]) for i = 1:length(R)] for ii = 1:3] # this gets abs of each term at each iteration\n",
    "    \n",
    "    # tests \n",
    "    @test norm(R[end])<1e-10 \n",
    "    \n",
    "    # convergence plotting \n",
    "    plot(Rp[1],yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "         yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "         title = \"Convergence of Newton's Method (3D case)\",label = \"|r_1|\")\n",
    "    plot!(Rp[2],label = \"|r_2|\")\n",
    "    display(plot!(Rp[3],label = \"|r_3|\"))\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-planet",
   "metadata": {},
   "source": [
    "## Part (c): Newtons method in optimization (4 pt)\n",
    "Now let's look at how we can use Newton's method in numerical optimization. Let's start by plotting a cost function $f(x)$, where $x\\in \\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    Q = [1.65539  2.89376; 2.89376  6.51521];\n",
    "    q = [2;-3]\n",
    "    f(x) = 0.5*x'*Q*x + q'*x + exp(-1.3*x[1] + 0.3*x[2]^2) # cost function \n",
    "    contour(-1:.1:1,-1:.1:1, (x1,x2)-> f([x1;x2]),title = \"Cost Function\",\n",
    "            xlabel = \"X_1\", ylabel = \"X_2\",fill = true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-mother",
   "metadata": {},
   "source": [
    "To find the minimum for this cost function $f(x)$, let's write the KKT conditions for optimality:\n",
    "\n",
    "\n",
    "$$ \\nabla f(x) = 0 \\quad \\quad \\text{stationarity}, $$\n",
    "\n",
    "\n",
    "which we see is just another rootfinding problem. We are now going to use Newton's method on the KKT conditions to find the $x$ in which $\\nabla f(x) = 0$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"2c\" begin \n",
    "    Q = [1.65539  2.89376; 2.89376  6.51521];\n",
    "    q = [2;-3]\n",
    "    f(x) = 0.5*x'*Q*x + q'*x + exp(-1.3*x[1] + 0.3*x[2]^2)\n",
    "    \n",
    "    function kkt_conditions(x)\n",
    "        # TODO: return the stationarity condition for the cost function f (∇f(x))\n",
    "        # hint: use forward diff\n",
    "        return zeros(length(x))\n",
    "    end\n",
    "    \n",
    "    residual_fx(_x) = kkt_conditions(_x)\n",
    "\n",
    "    x0 = [-0.9512129986081451, 0.8061342694354091]\n",
    "    X = newtons_method(x0, residual_fx; max_iters = 10)\n",
    "    R = residual_fx.(X) # the . evaluates the function at each element of the array\n",
    "\n",
    "    Rp = [[abs(R[i][ii]) for i = 1:length(R)] for ii = 1:length(R[1])] # this gets abs of each term at each iteration\n",
    "    \n",
    "    # tests \n",
    "    @test norm(R[end])<1e-10; \n",
    "\n",
    "    plot(Rp[1],yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "         yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "         title = \"Convergence of Newton's Method on KKT Conditions\",label = \"|r_1|\")\n",
    "    display(plot!(Rp[2],label = \"|r_2|\"))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-scene",
   "metadata": {},
   "source": [
    "## Note on Newton's method for unconstrained optimization\n",
    "To solve the above problem, we used Newton's method on the following equation:\n",
    "\n",
    "$$ \\nabla f(x) = 0 \\quad \\quad \\text{stationarity}, $$\n",
    "\n",
    "Which results in the following Newton steps:\n",
    "\n",
    "$$ \\Delta x = - \\bigg[ \\frac{\\partial \\nabla f(x)}{x} \\bigg]^{-1} \\nabla f(x_k). $$\n",
    "\n",
    "The jacobian of the gradient of $f(x)$ is the same as the hessian of $f(x)$ (write this out and convince yourself). This means we can rewrite the Newton step as the equivalent expression:\n",
    "\n",
    "$$ \\Delta x = - [\\nabla^2f(x)]^{-1} \\nabla f(x_k) $$\n",
    "\n",
    "What is the interpretation of this? Well, if we take a second order Taylor series of our cost function, and minimize this quadratic approximation of our cost function, we get the following optimization problem:\n",
    "\n",
    "$$ \\min_{\\Delta x} \\quad \\quad f(x_k) + [\\nabla f(x_k)^T] \\Delta x + \\frac{1}{2} \\Delta x^T [\\nabla^2f(x_k)] \\Delta x $$\n",
    "\n",
    "Where our optimality condition is the following:\n",
    "\n",
    "$$ \\nabla f(x_k)^T +  [\\nabla^2f(x_k)] \\Delta x = 0 $$ \n",
    "\n",
    "And we can solve for $\\Delta x$ with the following:\n",
    "\n",
    "$$ \\Delta x = - [\\nabla^2f(x)]^{-1} \\nabla f(x_k) $$\n",
    "\n",
    "Which is our Newton step. This means that Newton's method on the stationary condition is the same as minimizing the quadratic approximation of the cost function at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nearby-sentence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.7",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
